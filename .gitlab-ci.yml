---
stages:
 - test
 - build
 - staging
 - data-integration-staging
 - production
 - data-integration-production

# Image source available on https://forgemia.inra.fr/urgi-is/docker-rare
# It contains a JDK 8 and a Chrome browser
# Node, NPM and Yarn are installed by Gradle
image: urgi/docker-browsers:latest

# Disable the Gradle daemon for Continuous Integration servers as correctness
# is usually a priority over speed in CI environments. Using a fresh
# runtime for each build is more reliable since the runtime is completely
# isolated from any previous builds.
variables:
 GRADLE_OPTS: "-Dorg.gradle.daemon=false"
 GRADLE_USER_HOME: $CI_PROJECT_DIR/.gradle
 GIT_LFS_SKIP_SMUDGE: "1"

test:
 stage: test
 # the backend tests need an elasticsearch instance
 services:
   # even if that would be ideal
   # we can't just launch the service with just elasticsearch:6.3.1
   # because we need to pass some variables, but they are passed to _all_ containers
   # so they fail the start of other docker images like urgi/docker-browsers
   # the only solution is to override the entrypoint of the service and pass the arguments manually
   - name: docker.elastic.co/elasticsearch/elasticsearch:6.5.4
     alias: elasticsearch
     # discovery.type=single-node
     # single-node is necessary to start in development mode
     # so there will be no bootstrap checks that would fail on CI
     # especially the error regarding
     # `max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]`
     command: ["bin/elasticsearch", "-Ediscovery.type=single-node"]
 script: ./gradlew --parallel test
 cache:
   key: "$CI_COMMIT_REF_NAME"
   policy: pull-push
   paths:
     - .gradle
     - frontend/.gradle/
     - frontend/node_modules/
 artifacts:
  reports:
   junit:
    - ./backend/build/test-results/test/TEST-*.xml
    - ./frontend/karma-junit-tests-report/TEST*.xml
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*

lint:
 stage: test
 script: ./gradlew :frontend:lint

.build-app: &build_app
 # Hidden job which serves as a template for following
 # jobs below. See https://docs.gitlab.com/ee/ci/yaml/#anchors
 stage: build
 script:
  - ./gradlew assemble -Papp=${APP_NAME}
 artifacts:
  paths:
   - backend/build/libs/${APP_NAME}.jar
  expire_in: 1 week
 cache:
  key: "${CI_COMMIT_REF_NAME}"
  policy: pull-push
  paths:
   - .gradle
   - frontend/.gradle/
   - frontend/node_modules/
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*

build-rare:
 variables:
  APP_NAME: ${RARE_APP_NAME}
 <<: *build_app           # Merge the contents of the 'build_app' alias

build-wheatis:
 variables:
  APP_NAME: ${WHEATIS_APP_NAME}
 <<: *build_app           # Merge the contents of the 'build_app' alias

build-data-discovery:
 variables:
  APP_NAME: ${DATADISCOVERY_APP_NAME}
 <<: *build_app           # Merge the contents of the 'build_app' alias

.ssh-initialization-before-script: &ssh-initialization-before-script
 before_script:
  # SSH initialization
  - eval $(ssh-agent -s)
  - ssh-add <(echo "${SSH_PRIVATE_KEY}")
  - ssh -o StrictHostKeyChecking=no root@${SERVER_IP} 'echo "Successfully connected on $(hostname)"'
  #- ssh root@${SERVER_IP} 'service elasticsearch status || service elasticsearch restart'

.deploy-to-vm: &deploy_to_vm
 # Hidden job which serves as template for executed jobs below.
 # See https://docs.gitlab.com/ee/ci/yaml/#anchors
 retry: 2
 <<: *ssh-initialization-before-script
 script:
  # Copy jar to the server
  - scp ./backend/build/libs/${APP_NAME}.jar root@${SERVER_IP}:/opt/bootapp/${APP_NAME}-${ENV}.jar
  # Restarting service with the updated jar and the according Spring profiles enabled
  - ssh root@${SERVER_IP} "systemctl restart bootapp@${APP_NAME}-${ENV}"
  - eval $(ssh-agent -k)
  - echo "Deploy and index done. Application should be available at http://${SERVER_IP}:${APP_PORT}/${APP_CONTEXT}"

.variables-rare-beta: &variables-rare-beta
 variables:
  ENV: beta
  APP_NAME: rare
  APP_PORT: ${BETA_RARE_PORT}
  SERVER_IP: ${SERVER_IP}
  ES_HOST: ${ES_DEV_HOST}
  ES_PORT: ${ES_DEV_PORT}
  APP_CONTEXT: ${APP_NAME}-${ENV}

.variables-rare-staging: &variables-rare-staging
 variables:
  ENV: staging
  APP_NAME: rare
  APP_PORT: ${STAGING_RARE_PORT}
  SERVER_IP: ${SERVER_IP}
  ES_HOST: ${ES_PROD_HOST}
  ES_PORT: ${ES_PROD_PORT}
  APP_CONTEXT: ${APP_NAME}-${ENV}

.variables-rare-prod: &variables-rare-prod
 variables:
  ENV: prod
  APP_NAME: rare
  APP_PORT: ${PROD_RARE_PORT}
  SERVER_IP: ${SERVER_IP}
  ES_HOST: ${ES_PROD_HOST}
  ES_PORT: ${ES_PROD_PORT}
  APP_CONTEXT: ${APP_NAME}

.variables-wheatis-beta: &variables-wheatis-beta
 variables:
  ENV: beta
  APP_NAME: wheatis
  APP_PORT: ${BETA_WHEATIS_PORT}
  SERVER_IP: ${SERVER_IP}
  ES_HOST: ${ES_DEV_HOST}
  ES_PORT: ${ES_DEV_PORT}
  APP_CONTEXT: ${APP_NAME}-${ENV}

.variables-wheatis-staging: &variables-wheatis-staging
 variables:
  ENV: staging
  APP_NAME: wheatis
  APP_PORT: ${STAGING_WHEATIS_PORT}
  SERVER_IP: ${SERVER_IP}
  ES_HOST: ${ES_PROD_HOST}
  ES_PORT: ${ES_PROD_PORT}
  APP_CONTEXT: ${APP_NAME}-${ENV}

.variables-wheatis-prod: &variables-wheatis-prod
 variables:
  ENV: prod
  APP_NAME: wheatis
  APP_PORT: ${PROD_WHEATIS_PORT}
  SERVER_IP: ${SERVER_IP}
  APP_CONTEXT: ${APP_NAME}

.variables-data-discovery-beta: &variables-data-discovery-beta
 variables:
  ENV: beta
  APP_NAME: data-discovery
  APP_PORT: ${BETA_DATADISCOVERY_PORT}
  SERVER_IP: ${SERVER_IP}
  ES_HOST: ${ES_DEV_HOST}
  ES_PORT: ${ES_DEV_PORT}
  APP_CONTEXT: data-discovery-${ENV}

.variables-data-discovery-staging: &variables-data-discovery-staging
 variables:
  ENV: staging
  APP_NAME: data-discovery
  APP_PORT: ${STAGING_DATADISCOVERY_PORT}
  SERVER_IP: ${SERVER_IP}
  ES_HOST: ${ES_PROD_HOST}
  ES_PORT: ${ES_PROD_PORT}
  APP_CONTEXT: data-discovery-${ENV}

.variables-data-discovery-prod: &variables-data-discovery-prod
 variables:
  ENV: prod
  APP_NAME: data-discovery
  APP_PORT: ${PROD_DATADISCOVERY_PORT}
  SERVER_IP: ${SERVER_IP}
  ES_HOST: ${ES_PROD_HOST}
  ES_PORT: ${ES_PROD_PORT}
  APP_CONTEXT: data-discovery

deploy-rare-to-beta:
 stage: staging
 <<: *variables-rare-beta
 <<: *deploy_to_vm
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*
  refs:
   - branches
 except:
  refs:
   - master
 allow_failure: false # mandatory to block the execution of the pipeline

deploy-rare-to-staging:
 stage: staging
 <<: *variables-rare-staging
 <<: *deploy_to_vm
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*
  refs:
   - master

deploy-rare-to-prod:
 stage: production
 <<: *variables-rare-prod
 <<: *deploy_to_vm
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*
  refs:
   - master
 when: manual
 allow_failure: false

deploy-wheatis-to-beta:
 stage: staging
 <<: *variables-wheatis-beta
 <<: *deploy_to_vm
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*
  refs:
   - branches
 except:
  refs:
   - master
 allow_failure: false

deploy-wheatis-to-staging:
 stage: staging
 <<: *variables-wheatis-staging
 <<: *deploy_to_vm
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*
  refs:
   - master

deploy-wheatis-to-prod:
 stage: production
 <<: *variables-wheatis-prod
 <<: *deploy_to_vm
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*
  refs:
   - master
 when: manual
 allow_failure: false

deploy-data-discovery-to-beta:
 stage: staging
 <<: *variables-data-discovery-beta
 <<: *deploy_to_vm
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*
  refs:
   - branches
 except:
  refs:
   - master
 allow_failure: false

deploy-data-discovery-to-staging:
 stage: staging
 <<: *variables-data-discovery-staging
 <<: *deploy_to_vm
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*
  refs:
   - master

deploy-data-discovery-to-prod:
 stage: production
 <<: *variables-data-discovery-prod
 <<: *deploy_to_vm
 only:
  changes:
   - backend/src/**/*
   - frontend/**/*
  refs:
   - master
 when: manual
 allow_failure: false

.data-integration: &data-integration
# variables:
#  GIT_LFS_SKIP_SMUDGE: "0"
 retry: 1
 <<: *ssh-initialization-before-script
 script:
  - sleep 5
  # Do fetch of GIT LFS files only for data integration steps
  - git config lfs.fetchinclude "*"
  - git checkout $CI_COMMIT_REF_NAME
  #- "code=1 ; while [ $code != 0 ] ; do sleep 1 ; curl -s -u ${USER}:{PASSWORD} ${SERVER_IP}:${APP_PORT}/${APP_NAME}/actuator/info > /dev/null ; code=$? ; done ;"
  # Clean then copy data to the server and give the rights to bootapp
  - "ssh root@${SERVER_IP} \"rm -rf /tmp/${APP_NAME}-${ENV}/resources ; mkdir -p /tmp/${APP_NAME}-${ENV}/resources\""
  #- "rsync -a --progress ./data/${APP_NAME}/*gz root@${SERVER_IP}:/tmp/${APP_NAME}-${ENV}/resources/"
  - "scp ./data/${APP_NAME}/*gz root@${SERVER_IP}:/tmp/${APP_NAME}-${ENV}/resources/"
  - "ssh root@${SERVER_IP} \"parallel --bar 'gzip -d -f' :::  /tmp/${APP_NAME}-${ENV}/resources/*.gz\""
  - "ssh root@${SERVER_IP} \"chown bootapp:bootapp -R /tmp/${APP_NAME}-${ENV}/resources/\""
  # Deleting old indexes before reindexing from scratch
  - "curl -s -XDELETE ${ES_HOST}:${ES_PORT}/${APP_NAME}-${ENV}-* ; echo"
  # Should better handle those index switches by using the strategy proposed in the README file
  - "./scripts/createIndexAndAliases4CI.sh -host ${ES_HOST} -app ${APP_NAME} -env ${ENV} ; sleep 1 ; echo"
  - "./scripts/harvestCI.sh -url http://${SERVER_IP}:${APP_PORT}/${APP_CONTEXT} -app ${APP_NAME} -env ${ENV} ; echo"
 environment:
  # take care that some variables cannot be expanded according to where they are
  # defined. Info: https://forgemia.inra.fr/help/ci/variables/where_variables_can_be_used.md
  name: ${ENV}-${APP_NAME}
  # DO NOT specify environment's url here if it is already defined in the
  # Environments settings, it would cause the build not being visible in the
  # Deployments view because of a known bug:
  # https://gitlab.com/gitlab-org/gitlab-ce/issues/26537
 when: manual

data-integration-rare-beta:
 stage: data-integration-staging
 <<: *variables-rare-beta
 <<: *data-integration
 only:
  changes:
   - data/**/*.json.gz
  refs:
   - branches
 except:
  refs:
   - master

data-integration-rare-staging:
 stage: data-integration-staging
 <<: *variables-rare-staging
 <<: *data-integration
 only:
  changes:
   - data/**/*.json.gz
  refs:
   - master

data-integration-rare-prod:
 stage: data-integration-production
 <<: *variables-rare-prod
 <<: *data-integration
 only:
  changes:
   - data/**/*.json.gz
  refs:
   - master

data-integration-wheatis-beta:
 stage: data-integration-staging
 <<: *variables-wheatis-beta
 <<: *data-integration
 only:
  changes:
   - data/**/*.json.gz
  refs:
   - branches
 except:
  refs:
   - master

data-integration-wheatis-staging:
 stage: data-integration-staging
 <<: *variables-wheatis-staging
 <<: *data-integration
 only:
  changes:
   - data/**/*.json.gz
  refs:
   - master

data-integration-wheatis-prod:
 stage: data-integration-production
 <<: *variables-wheatis-prod
 <<: *data-integration
 only:
  changes:
   - data/**/*.json.gz
  refs:
   - master

data-integration-data-discovery-beta:
 stage: data-integration-staging
 <<: *variables-data-discovery-beta
 <<: *data-integration
 only:
  changes:
   - data/**/*.json.gz
  refs:
   - branches
 except:
  refs:
   - master

data-integration-data-discovery-staging:
 stage: data-integration-staging
 <<: *variables-data-discovery-staging
 <<: *data-integration
 only:
  changes:
   - data/**/*.json.gz
  refs:
   - master

data-integration-data-discovery-prod:
 stage: data-integration-production
 <<: *variables-data-discovery-prod
 <<: *data-integration
 only:
  changes:
   - data/**/*.json.gz
  refs:
   - master

# Should find a way to authorize such pipeline once at a time in order to avoid
# multiple restart of services, elasticsearch cluster or reindexing
