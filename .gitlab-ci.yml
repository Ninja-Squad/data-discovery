---
stages:
 - test
 - build
 - staging
 - data-integration-staging
 - production
 - data-integration-production

# Image source available on https://github.com/Ninja-Squad/docker-rare
# It contains a JDK 8 and a Chrome browser
# Node, NPM and Yarn are installed by Gradle
image: ninjasquad/docker-rare

# Disable the Gradle daemon for Continuous Integration servers as correctness
# is usually a priority over speed in CI environments. Using a fresh
# runtime for each build is more reliable since the runtime is completely
# isolated from any previous builds.
variables:
 GRADLE_OPTS: "-Dorg.gradle.daemon=false"

before_script:
 - export GRADLE_USER_HOME=`pwd`/.gradle


test:
 stage: test
 # the backend tests need an elasticsearch instance
 services:
   # even if that would be ideal
   # we can't just launch the service with just elasticsearch:6.3.1
   # because we need to pass some variables, but they are passed to _all_ containers
   # so they fail the start of other docker images like ninjasquad/docker-rare
   # the only solution is to override the entrypoint of the service and pass the arguments manually
   - name: docker.elastic.co/elasticsearch/elasticsearch:6.3.1
     alias: elasticsearch
     # discovery.type=single-node
     # single-node is necessary to start in development mode
     # so there will be no bootstrap checks that would fail on CI
     # especially the error regarding
     # `max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]`
     # cluster.name=es-rare
     # the cluster name used in tests
     command: ["bin/elasticsearch", "-Ediscovery.type=single-node", "-Ecluster.name=es-data-discovery"]
 script: ./gradlew test --parallel
 cache:
   key: "$CI_COMMIT_REF_NAME"
   policy: pull
   paths:
     - build
     - .gradle
     - buildSrc
     - frontend/.gradle/
     - frontend/node_modules/
 artifacts:
  reports:
   junit: 
    - ./backend/build/test-results/test/TEST-*.xml
    - ./frontend/karma-junit-tests-report/TEST*.xml



.build-app: &build_app
 # Hidden job which serves as a template for 2 following
 # jobs below. See https://docs.gitlab.com/ee/ci/yaml/#anchors
 stage: build
 script:
  # Tests already executed at previous stage, skipping them now
  - ./gradlew assemble -Papp=${APP_NAME}
 artifacts:
  paths:
   - backend/build/libs/${APP_NAME}.jar
  expire_in: 1 week
 cache:
  key: "${CI_COMMIT_REF_NAME}"
  policy: pull
  paths:
   - build
   - buildSrc
   - .gradle
   - frontend/.gradle/
   - frontend/node_modules/

build-rare:
 variables:
  APP_NAME: ${RARE_APP_NAME}
 <<: *build_app           # Merge the contents of the 'build_app' alias

build-wheatis:
 variables:
  APP_NAME: ${WHEATIS_APP_NAME}
 <<: *build_app           # Merge the contents of the 'build_app' alias

.ssh-initialization-before-script: &ssh-initialization-before-script
 before_script:
  # SSH initialization
  - eval $(ssh-agent -s)
  - ssh-add <(echo "${SSH_PRIVATE_KEY}")
  - ssh -o StrictHostKeyChecking=no root@${SERVER_IP} 'echo "Successfully connected on $(hostname)"'
  - ssh root@${SERVER_IP} 'service elasticsearch status || service elasticsearch restart'
  # Installing missing dependencies
  - "apt-get update -qq && apt-get install -qy curl"

.deploy-to-vm: &deploy_to_vm
 # Hidden job which serves as template for 2 executed jobs below.
 # See https://docs.gitlab.com/ee/ci/yaml/#anchors
 retry: 2
 <<: *ssh-initialization-before-script
 script:
  # Copy jar and data (cleaning them before) to the server
  - "scp ./backend/build/libs/${APP_NAME}.jar root@${SERVER_IP}:/opt/${APP_NAME}/${APP_NAME}-${ENV}.jar"
  - "ssh root@${SERVER_IP} \"rm -rf /tmp/${APP_NAME}-${ENV}/resources ; mkdir -p /tmp/${APP_NAME}-${ENV}/resources\""
  - "scp ./data/${APP_NAME}/*gz root@${SERVER_IP}:/tmp/${APP_NAME}-${ENV}/resources/"
  - "ssh root@${SERVER_IP} \"gunzip -rv /tmp/${APP_NAME}-${ENV}/resources\""
  # Restarting service with the updated jar and the according Spring profiles enabled
  - "ssh root@${SERVER_IP} \"service ${APP_NAME}-${ENV} restart\""
  # Waiting embedded tomcat starts...
  # Should find a better way than a shitty sleep...
  - eval $(ssh-agent -k)
  - "echo \"Deploy and index done. Application should be available at: http://${SERVER_IP}:${APP_PORT}/${APP_NAME}-${ENV} \""

.variables-rare-beta: &variables-rare-beta
 variables:
  ENV: beta
  APP_NAME: rare
  APP_PORT: ${BETA_RARE_PORT}
  SERVER_IP: ${BETA_SERVER_IP}

.variables-rare-staging: &variables-rare-staging
 variables:
  ENV: staging
  APP_NAME: rare
  APP_PORT: ${STAGING_RARE_PORT}
  SERVER_IP: ${BETA_SERVER_IP}

.variables-rare-prod: &variables-rare-prod
 variables:
  ENV: prod
  APP_NAME: rare
  APP_PORT: ${PROD_RARE_PORT}
  SERVER_IP: ${BETA_SERVER_IP}

.variables-wheatis-beta: &variables-wheatis-beta
 variables:
  ENV: beta
  APP_NAME: wheatis
  APP_PORT: ${BETA_WHEATIS_PORT}
  SERVER_IP: ${BETA_SERVER_IP}

.variables-wheatis-staging: &variables-wheatis-staging
 variables:
  ENV: staging
  APP_NAME: wheatis
  APP_PORT: ${STAGING_WHEATIS_PORT}
  SERVER_IP: ${BETA_SERVER_IP}

.variables-wheatis-prod: &variables-wheatis-prod
 variables:
  ENV: prod
  APP_NAME: wheatis
  APP_PORT: ${PROD_WHEATIS_PORT}
  SERVER_IP: ${BETA_SERVER_IP}

deploy-rare-to-beta:
 stage: staging
 <<: *variables-rare-beta
 <<: *deploy_to_vm
 only: 
  - branches
 except:
  - master
 allow_failure: false # mandatory to block the execution of the pipeline

deploy-rare-to-staging:
 stage: staging
 <<: *variables-rare-staging
 <<: *deploy_to_vm
 only:
  - master

deploy-rare-to-prod:
 stage: production
 <<: *variables-rare-prod
 <<: *deploy_to_vm
 only:
  - master
 when: manual
 allow_failure: false

deploy-wheatis-to-beta:
 stage: staging
 <<: *variables-wheatis-beta
 <<: *deploy_to_vm
 only:
  - branches
 except:
  - master
 allow_failure: false

deploy-wheatis-to-staging:
 stage: staging
 <<: *variables-wheatis-staging
 <<: *deploy_to_vm
 only:
  - master

deploy-wheatis-to-prod:
 stage: production
 <<: *variables-wheatis-prod
 <<: *deploy_to_vm
 only:
  - master
 when: manual
 allow_failure: false
 
.data-integration: &data-integration
 retry: 1
 <<: *ssh-initialization-before-script
 script:
  - sleep 5
  # - "code=1 ; while [ $code != 0 ] ; do sleep 1 ; curl -s -u ${USER}:{PASSWORD} ${SERVER_IP}:${APP_PORT}/${APP_NAME}/actuator/info > /dev/null ; code=$? ; done ;"
  # Deleting old indexes before reindexing from scratch
  - "curl -s -XDELETE ${SERVER_IP}:9200/${APP_NAME}-${ENV}-* ; echo"
  # Should better handle those index switches by using the strategy proposed
  # in the README file
  - "./scripts/createIndexAndAliases4CI.sh ${SERVER_IP} ${APP_NAME} ${ENV} ; sleep 1 ; echo"
  - "./scripts/harvestCI.sh ${SERVER_IP} ${APP_PORT} ${APP_NAME} ${ENV} ; echo"
 environment:
  # take care that some variables cannot be expanded according to where they are
  # defined. Info: https://forgemia.inra.fr/help/ci/variables/where_variables_can_be_used.md
  name: ${ENV}-${APP_NAME}
  # DO NOT specify environment's url here if it is already defined in the
  # Environments settings, it would cause the build not being visible in the
  # Deployments view because of a known bug:
  # https://gitlab.com/gitlab-org/gitlab-ce/issues/26537

data-integration-rare-beta:
 stage: data-integration-staging
 <<: *variables-rare-beta
 <<: *data-integration
 only:
  - branches
 except:
  - master

data-integration-rare-staging:
 stage: data-integration-staging
 <<: *variables-rare-staging
 <<: *data-integration
 only:
  - master

data-integration-rare-prod:
 stage: data-integration-production
 <<: *variables-rare-prod
 <<: *data-integration
 only:
  - master
 when: manual

data-integration-wheatis-beta:
 stage: data-integration-staging
 <<: *variables-wheatis-beta
 <<: *data-integration
 only:
  - branches
 except:
  - master

data-integration-wheatis-staging:
 stage: data-integration-staging
 <<: *variables-wheatis-staging
 <<: *data-integration
 only:
  - master

data-integration-wheatis-prod:
 stage: data-integration-production
 <<: *variables-wheatis-prod
 <<: *data-integration
 only:
  - master
 when: manual

# Should find a way to authorize such pipeline once at a time in order to avoid
# multiple restart of services, elasticsearch cluster or reindexing
